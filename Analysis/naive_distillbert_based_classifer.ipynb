{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Rotman Data Science Competition\n",
    "### A no-brainer implementation of DistilBERT for prediction of substitutes to sold out goods when purchasing via instacart\n",
    "Model created by fine-tuning DistilBERT on the MMA competition dataset using methods described by huggingface nlp tutorial.\n",
    "### Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, DataCollatorForLanguageModeling, TrainingArguments, Trainer, pipeline, default_data_collator\n",
    "from datasets import load_dataset\n",
    "import collections\n",
    "import pandas as pd\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Preprocessing\n",
    "### Turning our data into format for text masking"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "   order_id  product_id                                   product_name  \\\n0         1       49302                               Bulgarian Yogurt   \n1         1       11109  Organic 4% Milk Fat Whole Milk Cottage Cheese   \n2         1       10246                          Organic Celery Hearts   \n3         1       49683                                 Cucumber Kirby   \n4         1       43633           Lightly Smoked Sardines in Olive Oil   \n\n   aisle_id                 aisle  department_id    department  \n0       120                yogurt             16    dairy eggs  \n1       108  other creams cheeses             16    dairy eggs  \n2        83      fresh vegetables              4       produce  \n3        83      fresh vegetables              4       produce  \n4        95   canned meat seafood             15  canned goods  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>order_id</th>\n      <th>product_id</th>\n      <th>product_name</th>\n      <th>aisle_id</th>\n      <th>aisle</th>\n      <th>department_id</th>\n      <th>department</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>49302</td>\n      <td>Bulgarian Yogurt</td>\n      <td>120</td>\n      <td>yogurt</td>\n      <td>16</td>\n      <td>dairy eggs</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>11109</td>\n      <td>Organic 4% Milk Fat Whole Milk Cottage Cheese</td>\n      <td>108</td>\n      <td>other creams cheeses</td>\n      <td>16</td>\n      <td>dairy eggs</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>10246</td>\n      <td>Organic Celery Hearts</td>\n      <td>83</td>\n      <td>fresh vegetables</td>\n      <td>4</td>\n      <td>produce</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>49683</td>\n      <td>Cucumber Kirby</td>\n      <td>83</td>\n      <td>fresh vegetables</td>\n      <td>4</td>\n      <td>produce</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>43633</td>\n      <td>Lightly Smoked Sardines in Olive Oil</td>\n      <td>95</td>\n      <td>canned meat seafood</td>\n      <td>15</td>\n      <td>canned goods</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_PATH = \"./data/mma_mart.csv\"\n",
    "raw_data = pd.read_csv(DATA_PATH)\n",
    "raw_data.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "outputs": [],
   "source": [
    "products = raw_data.loc[:, 'product_id' : 'product_name']\n",
    "products.drop_duplicates(\"product_id\", inplace=True)\n",
    "products.set_index(\"product_id\", inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "data": {
      "text/plain": "   order_id  product_id                                   product_name\n0         1       49302                               Bulgarian Yogurt\n1         1       11109  Organic 4% Milk Fat Whole Milk Cottage Cheese\n2         1       10246                          Organic Celery Hearts\n3         1       49683                                 Cucumber Kirby\n4         1       43633           Lightly Smoked Sardines in Olive Oil",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>order_id</th>\n      <th>product_id</th>\n      <th>product_name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>49302</td>\n      <td>Bulgarian Yogurt</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>11109</td>\n      <td>Organic 4% Milk Fat Whole Milk Cottage Cheese</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>10246</td>\n      <td>Organic Celery Hearts</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>49683</td>\n      <td>Cucumber Kirby</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>43633</td>\n      <td>Lightly Smoked Sardines in Olive Oil</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = raw_data.loc[:, 'order_id':'product_name']\n",
    "raw_data.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "# Put orders into string format\n",
    "FORMAT = \"On order {x} the user bought {item 1, ... , item y}.\"\n",
    "\n",
    "# raw_data[\"str_p_id\"] = [f\"(id {str(i)})\" for i in raw_data[\"product_id\"]]\n",
    "# raw_data[\"name_id\"] = raw_data[\"product_name\"] + raw_data[\"str_p_id\"]\n",
    "# raw_data.drop(columns=[\"product_id\", \"product_name\", \"str_p_id\"], inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "raw_data = raw_data.set_index(\"order_id\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "data": {
      "text/plain": "                                               order\n0  On order 1, the user purchased Bulgarian Yogur...\n1  On order 2, the user purchased Organic Egg Whi...\n2  On order 3, the user purchased Total 2% with S...\n3  On order 4, the user purchased Plain Pre-Slice...\n4  On order 5, the user purchased Bag of Organic ...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>order</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>On order 1, the user purchased Bulgarian Yogur...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>On order 2, the user purchased Organic Egg Whi...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>On order 3, the user purchased Total 2% with S...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>On order 4, the user purchased Plain Pre-Slice...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>On order 5, the user purchased Bag of Organic ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# formated_data = pd.DataFrame(columns=[\"order\"])\n",
    "# for idx in raw_data.index.unique():\n",
    "#     items = \", \".join(raw_data.loc[idx][\"product_name\"])\n",
    "#     formated_data.loc[idx, \"order\"] = f\"On order {idx}, the user purchased {items}\"\n",
    "# formated_data.head()\n",
    "# formated_data.to_csv(SAVE_PATH, index=False)\n",
    "\n",
    "SAVE_PATH = \"./data/mma_cart_preprocessed_naive\"\n",
    "\n",
    "formated_data = pd.read_csv(SAVE_PATH)\n",
    "formated_data.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Data into Huggingface Dataset object"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['order'],\n        num_rows: 78266\n    })\n    test: Dataset({\n        features: ['order'],\n        num_rows: 19567\n    })\n})"
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set = load_dataset(\"csv\", data_files=SAVE_PATH)\n",
    "data_set = data_set['train'].train_test_split(test_size=0.2)\n",
    "data_set"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/78266 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "647faff77dac41e4bb44b29172671050"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (648 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/19567 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "42a187a27b094472b7e948583eba5dbe"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'attention_mask', 'word_ids'],\n        num_rows: 78266\n    })\n    test: Dataset({\n        features: ['input_ids', 'attention_mask', 'word_ids'],\n        num_rows: 19567\n    })\n})"
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    result = tokenizer(examples[\"order\"])\n",
    "    if tokenizer.is_fast:\n",
    "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
    "    return result\n",
    "\n",
    "\n",
    "# Use batched=True to activate fast multithreading!\n",
    "tokenized_datasets = data_set.map(\n",
    "    tokenize_function, batched=True, remove_columns=[\"order\"]\n",
    ")\n",
    "tokenized_datasets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/78266 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "66c3ef7e3e814d96b9f2934db8d524a2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/19567 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d6fac8c42b9240f5a4b0cfbfcb5f639f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n        num_rows: 46266\n    })\n    test: Dataset({\n        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n        num_rows: 11500\n    })\n})"
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_size = 128\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    # Compute length of concatenated texts\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the last chunk if it's smaller than chunk_size\n",
    "    total_length = (total_length // chunk_size) * chunk_size\n",
    "    # Split by chunks of max_len\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # Create a new labels column\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "lm_datasets = tokenized_datasets.map(group_texts, batched=True)\n",
    "lm_datasets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [
    {
     "data": {
      "text/plain": "'silk pie, pasilla pepper, sharp cheddar cheese, natural sharp cheddar sliced cheese, cranberry juice, lemonade, classic soda, half and half [SEP] [CLS] on order 36247, the user purchased organic raspberries, ice cream, super premium, dutch chocolate, raspberry sorbet pops, stage 1 just peaches baby food, freshly squeezed orange juice [SEP] [CLS] on order 71365, the user purchased organic broccoli, organic hass avocado, organic honey sweet whole wheat bread, avocado roll, organic shredded mild cheddar, honey graham sticks, organic lemon'"
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(lm_datasets[\"train\"][1][\"input_ids\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [],
   "source": [
    "# data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "outputs": [],
   "source": [
    "wwm_probability = 0.2\n",
    "\n",
    "\n",
    "def whole_word_masking_data_collator(features):\n",
    "    for feature in features:\n",
    "        word_ids = feature.pop(\"word_ids\")\n",
    "\n",
    "        # Create a map between words and corresponding token indices\n",
    "        mapping = collections.defaultdict(list)\n",
    "        current_word_index = -1\n",
    "        current_word = None\n",
    "        for idx, word_id in enumerate(word_ids):\n",
    "            if word_id is not None:\n",
    "                if word_id != current_word:\n",
    "                    current_word = word_id\n",
    "                    current_word_index += 1\n",
    "                mapping[current_word_index].append(idx)\n",
    "\n",
    "        # Randomly mask words\n",
    "        mask = np.random.binomial(1, wwm_probability, (len(mapping),))\n",
    "        input_ids = feature[\"input_ids\"]\n",
    "        labels = feature[\"labels\"]\n",
    "        new_labels = [-100] * len(labels)\n",
    "        for word_id in np.where(mask)[0]:\n",
    "            word_id = word_id.item()\n",
    "            for idx in mapping[word_id]:\n",
    "                new_labels[idx] = labels[idx]\n",
    "                input_ids[idx] = tokenizer.mask_token_id\n",
    "        feature[\"labels\"] = new_labels\n",
    "\n",
    "    return default_data_collator(features)\n",
    "\n",
    "def whole_product_masking_data_collator(features, products: pd.DataFrame):\n",
    "    \"\"\" Mask whole products \"\"\"\n",
    "    for feature in features:\n",
    "        word_ids = feature.pop(\"word_ids\")\n",
    "\n",
    "        # Create a map between words and corresponding token indices\n",
    "        mapping = collections.defaultdict(list)\n",
    "        current_word_index = -1\n",
    "        current_word = None\n",
    "        for idx, word_id in enumerate(word_ids):\n",
    "            if word_id is not None:\n",
    "                if word_id != current_word:\n",
    "                    current_word = word_id\n",
    "                    current_word_index += 1\n",
    "                mapping[current_word_index].append(idx)\n",
    "\n",
    "        # Randomly mask words\n",
    "        mask = np.random.binomial(1, wwm_probability, (len(mapping),))\n",
    "        input_ids = feature[\"input_ids\"]\n",
    "        labels = feature[\"labels\"]\n",
    "        new_labels = [-100] * len(labels)\n",
    "        for word_id in np.where(mask)[0]:\n",
    "            word_id = word_id.item()\n",
    "            for idx in mapping[word_id]:\n",
    "                new_labels[idx] = labels[idx]\n",
    "                input_ids[idx] = tokenizer.mask_token_id\n",
    "        feature[\"labels\"] = new_labels\n",
    "\n",
    "    return default_data_collator(features)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'>>> [CLS] on [MASK] 67513, the user purchased [MASK] blueberries clamshell, [MASK] [MASK] cheese, mini seedless watermelon pack, organic strawberries, hass avocado variety [SEP] [CLS] on order 66048, the user purchased [MASK] [MASK] [MASK] whiskey, scoopable scented clumping cat litter [MASK] jalapeno pepper, pure leaf unsweetened [MASK] brewed [MASK] [MASK] beer [MASK] n bratwurst, lemon sparkling water [MASK] spicy brown mustard, [MASK] / [MASK] ground beef, [MASK] hamburger buns, double rolls bath [MASK], paper towels choose - a - [MASK], mega rolls, one - ply, chocolate'\n",
      "\n",
      "'>>> silk pie, pasilla pepper, [MASK] cheddar cheese, natural sharp cheddar sliced [MASK] [MASK] cranberry [MASK] [MASK] lemonade [MASK] classic [MASK] [MASK] half and half [SEP] [CLS] on [MASK] [MASK] [MASK] [MASK], the user purchased organic raspberries [MASK] ice cream, super [MASK] [MASK] dutch chocolate, raspberry sorbet pops, stage 1 [MASK] peaches baby food [MASK] freshly squeezed orange juice [SEP] [CLS] on order 71365, the user [MASK] organic broccoli, organic hass avocado, organic honey sweet whole wheat [MASK], avocado roll [MASK] [MASK] shredded mild [MASK] [MASK] [MASK], honey graham sticks [MASK] [MASK] lemon'\n"
     ]
    }
   ],
   "source": [
    "samples = [lm_datasets[\"train\"][i] for i in range(2)]\n",
    "batch = whole_word_masking_data_collator(samples)\n",
    "\n",
    "for chunk in batch[\"input_ids\"]:\n",
    "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n        num_rows: 40000\n    })\n    test: Dataset({\n        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n        num_rows: 4000\n    })\n})"
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = 40_000\n",
    "test_size = int(0.1 * train_size)\n",
    "\n",
    "downsampled_dataset = lm_datasets[\"train\"].train_test_split(\n",
    "    train_size=train_size, test_size=test_size, seed=42\n",
    ")\n",
    "downsampled_dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "# Show the training loss with every epoch\n",
    "logging_steps = len(downsampled_dataset[\"train\"]) // batch_size\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./substitute_classifier\",\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    push_to_hub=False,\n",
    "    logging_steps=logging_steps,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=downsampled_dataset[\"train\"],\n",
    "    eval_dataset=downsampled_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='1875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   2/1875 : < :, Epoch 0.00/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "CHECKPOINT_PATH = \"./substitute_classifier/naive_distillbert_classifier_checkpoint_40000\"\n",
    "trainer.train(CHECKPOINT_PATH)\n",
    "trainer.save_model(CHECKPOINT_PATH)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='1' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/63 : < :]\n    </div>\n    "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "{'eval_loss': 0.1854913979768753,\n 'eval_runtime': 52.2443,\n 'eval_samples_per_second': 76.563,\n 'eval_steps_per_second': 1.206,\n 'epoch': 3.0}"
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [],
   "source": [
    "my_model = AutoModelForMaskedLM.from_pretrained(CHECKPOINT_PATH)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "outputs": [],
   "source": [
    "my_pipeline = pipeline(\"fill-mask\", model=my_model, tokenizer=tokenizer, targets=[\"banana\", \"jerk\", \"energy\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "outputs": [],
   "source": [
    "sample_input = \"On order 50113, the user purchased Organic Peeled Whole Baby Carrots, Zen Tea, Sugar Free Energy Drink, Pretzel Crisps Original Deli Style Pretzel Crackers, Madagascar Vanilla Almond Nuts & Spices Bars, [MASK], Popped Corn Chips Kettle, 100 Calorie  Per Bag Popcorn, Sweet Chipotle Beef Jerky, Chili Lime Beef Jerky Pack, Roasted & Salted Shelled Pistachios, Sugar Free Gum Winter Frost, Brownie Brittle Chocolate Chip, Sunburst Tangerine, Double Chocolate Chip Cookies\"\n",
    "\n",
    "correct_answer = \"Crunchy Peanut Butter Energy Bar\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "outputs": [
    {
     "data": {
      "text/plain": "[{'score': 0.33547356724739075,\n  'token': 15212,\n  'token_str': 'banana',\n  'sequence': 'on order 50113, the user purchased organic peeled whole baby carrots, zen tea, sugar free energy drink, pretzel crisps original deli style pretzel crackers, madagascar vanilla almond nuts & spices bars, banana, popped corn chips kettle, 100 calorie per bag popcorn, sweet chipotle beef jerky, chili lime beef jerky pack, roasted & salted shelled pistachios, sugar free gum winter frost, brownie brittle chocolate chip, sunburst tangerine, double chocolate chip cookies'},\n {'score': 0.00013964167737867683,\n  'token': 2943,\n  'token_str': 'energy',\n  'sequence': 'on order 50113, the user purchased organic peeled whole baby carrots, zen tea, sugar free energy drink, pretzel crisps original deli style pretzel crackers, madagascar vanilla almond nuts & spices bars, energy, popped corn chips kettle, 100 calorie per bag popcorn, sweet chipotle beef jerky, chili lime beef jerky pack, roasted & salted shelled pistachios, sugar free gum winter frost, brownie brittle chocolate chip, sunburst tangerine, double chocolate chip cookies'}]"
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_pipeline(sample_input, top_k = 2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
